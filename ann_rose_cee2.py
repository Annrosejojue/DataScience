# -*- coding: utf-8 -*-
"""Ann Rose CEE2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1foSTzbhmk6JBZ-c94xEI1JAVVDuWqPL8
"""

import pandas as pd
import numpy as np
hospital_data = pd.read_csv('/content/hospital_readmission.csv')
hospital_data.head()

hospital_data.head(50)

"""print the data types of attributes"""

print("Data types of attributes:")
print(hospital_data.dtypes)

"""print the sum of missing values in the file."""

missing_values_sum = hospital_data.isnull().sum().sum()
print(f"\nSum of missing values in the file: {missing_values_sum}")

"""Print the count of patients who stayed in hospital for more than 10 days. (time_in_hospital)"""

patients_more_than_10_days = hospital_data[hospital_data['time_in_hospital'] > 10]
count_patients_more_than_10_days = patients_more_than_10_days.shape[0]
print(f"Number of patients who stayed in hospital for more than 10 days: {count_patients_more_than_10_days}")

"""Drop features that have large missing values."""

missing_value_counts = hospital_data.isin(['?']).sum()

# Step 3: Print the count of missing values in each column
print("Count of missing values ('?') in each column:")
print(missing_value_counts)

""" Drop features that have large missing values."""

columns_to_drop = ['weight']
hospital_data_cleaned = hospital_data.drop(columns=columns_to_drop)

# Step 4: Optionally, save the cleaned dataset to a new CSV file
cleaned_file_path = 'hospital_readmissions_cleaned.csv'
hospital_data_cleaned.to_csv(cleaned_file_path, index=False)

# Step 5: Optionally, print summary of cleaned dataset
print(f"\nSummary of cleaned dataset:")
print(hospital_data_cleaned.info())

hospital_data_cleaned.head(10)

"""  Print the top 5 medical specialties in the hospital and respective count of readmissions"""

top_specialties = hospital_data_cleaned.groupby('medical_specialty')['readmitted'].count().nlargest(5)

# Step 3: Print the top 5 medical specialties and respective count of readmissions
print("Top 5 medical specialties and respective count of readmissions:")
print(top_specialties)

""" Print the correlation between time in hospital and number of lab procedures"""

# Step 2: Calculate correlation between time_in_hospital and num_lab_procedures
correlation = hospital_data['time_in_hospital'].corr(hospital_data['num_lab_procedures'])

# Step 3: Print the correlation coefficient
print(f"Correlation between time in hospital and number of lab procedures: {correlation}")

"""Subset data based on the following features, apply PCA on them and reduce. List the components that contribute to 85% variation."""

print("Column names:", hospital_data_cleaned.columns.tolist())

# prompt: subset_data = hospital_data[['num_lab_procedures', 'num_procedures', 'num_medications', 'number_outpatient', 'number_emergency', 'number_inpatient', 'diag_1', 'diag_2', 'diag_3', 'diag_4', 'diag_5', 'number_diagnoses']]

# Create a subset of the data with the specified columns
subset_data = hospital_data_cleaned[['num_lab_procedures', 'num_procedures', 'num_medications', 'number_outpatient', 'number_emergency', 'number_inpatient', 'diag_1', 'diag_2', 'diag_3', 'diag_4', 'diag_5', 'number_diagnoses']]

# Display the first few rows of the subset
print(subset_data.head())

# Identify numerical columns
numerical_columns = subset_data.select_dtypes(include=[np.number]).columns

# Extract predictors (exclude non-numerical columns)
DataFrame_predictors = subset_data[numerical_columns]

# Scale the predictors
scaler = StandardScaler()
scaled_predictors = scaler.fit_transform(DataFrame_predictors)

from sklearn.decomposition import PCA
# Perform PCA and explain 85% of the variance
pca = PCA(n_components=0.85)  # Adjust the threshold accordingly
pca_out = pca.fit_transform(scaled_predictors)

# Display results of PCA
explained_variance_ratio = pca.explained_variance_ratio_
cumulative_variance_ratio = np.cumsum(explained_variance_ratio)

print("Explained Variance Ratio:", explained_variance_ratio)
print("Cumulative Variance Ratio:", cumulative_variance_ratio)

# List the components that contribute to 85% variation
components_85 = [i for i in range(len(explained_variance_ratio)) if cumulative_variance_ratio[i] < 0.85]
print("Components contributing to 85% variation:", components_85)

"""Question 3: Read the HosptialReadmissionHistory.csv file. The count of readmissions is provided from 2015 to 2023.  Forecast the readmissions from Jan to March of 2024 using ETS and ARIMA model. Present the performance of these models using MAPE%

Read the HosptialReadmissionHistory.csv file
"""

readmission_history = pd.read_csv('/content/HospitalReadmissionsHistory.csv')
readmission_history.head(10)

# Convert the 'Month' column to datetime format
readmission_history['Month'] = pd.to_datetime(readmission_history['Month'])

# Set the 'Month' column as the index
readmission_history.set_index('Month', inplace=True)

# Plot the readmissions data
readmission_history.plot(figsize=(12, 6))
plt.title('Hospital Readmissions from 2015 to 2023')
plt.xlabel('Date')
plt.ylabel('Count of Readmissions')
plt.show()

import pandas as pd
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.holtwinters import ExponentialSmoothing
from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_absolute_percentage_error

from sklearn.model_selection import train_test_split

# Split the data into training and testing sets
train_data, test_data = train_test_split(readmission_history['Count of readmissions'], test_size=0.2, shuffle=False)

# Fit the ETS model on the training data
ets_model = ExponentialSmoothing(train_data, seasonal='add', seasonal_periods=12).fit()

# Fit the ARIMA model on the training data (using automatic order selection)
arima_model = ARIMA(train_data, order=(1, 1, 1), seasonal_order=(1, 1, 1, 12)).fit()

# Forecast for January to March 2024 using the trained models
forecast_periods = 3
ets_forecast = ets_model.forecast(steps=forecast_periods)
arima_forecast = arima_model.forecast(steps=forecast_periods)

# Prepare the results in a DataFrame
forecast_df = pd.DataFrame({
    'Month': pd.date_range(start='2024-01-01', periods=forecast_periods, freq='MS'),
    'ETS Forecast': ets_forecast,
    'ARIMA Forecast': arima_forecast
})

# Calculate MAPE for ETS and ARIMA models
ets_mape = mean_absolute_percentage_error(test_data, ets_model.forecast(steps=len(test_data))) * 100
arima_mape = mean_absolute_percentage_error(test_data, arima_model.forecast(steps=len(test_data))) * 100

print(f'ETS MAPE: {ets_mape:.2f}%')
print(f'ARIMA MAPE: {arima_mape:.2f}%')